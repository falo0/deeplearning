{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Code Implementation Of Gradient Descent And RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function, Gradient, Starting Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following is a simple loss function for example purposes only: J(**theta**) = theta_1 \\* theta_2 \\* ... (multiplication of all elements). The loss function J is determined by the real values **y** and the predictions/classifications **y_hat**, which is a function of the predictors **x** (the data), and the parameters **theta** of the prediction model. E.g. loss J = mean(abs(f(**x**, **theta**) - **y**)) would be the Mean Absolute Error. To keep the example simple, we just assume a certain influence of **theta** without explaining how exactly **theta** effects the loss through the model f(**x**, **theta**) = **y_hat**. The given loss funciton does not converge to a minimum, but it has a saddle point at (0,0,...). In this example, the goal is to get close to the saddle point (while the real life goal with more complex loss functions is to get close to a minimum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function J\n",
    "def J(theta):\n",
    "    loss = np.prod(theta) #multiply all elements of theta, e.g. theta1*theta2*...\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Of J\n",
    "def dJ(theta):\n",
    "    gradient = theta * 0\n",
    "    for i in range(len(theta)):\n",
    "        gradient[i] = np.prod(theta[np.arange(len(theta)) != i])\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "[2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Setting The Starting Values Of Theta, Getting Loss And Gradient At Theata_Start\n",
    "theta_start = np.array([2., 2.])\n",
    "print(J(theta_start))\n",
    "print(dJ(theta_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms: Gradient Descent And RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(J, dJ, theta_start, steps = 100, lr = 0.001):\n",
    "    theta = np.copy(theta_start)\n",
    "    loss = []\n",
    "    for _ in range(steps):\n",
    "        gradient = dJ(theta)\n",
    "        theta -= lr * gradient\n",
    "        loss.append(J(theta))\n",
    "\n",
    "    return theta, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop(J, dJ, theta_start, steps = 100, lr = 0.001, decay = .9, delta = 1e-6):\n",
    "    theta = np.copy(theta_start)\n",
    "    loss = []\n",
    "    gradient_mean_sqr = np.zeros(theta.shape, dtype=float) # vector full of zeros\n",
    "\n",
    "    for _ in range(steps):\n",
    "        gradient = dJ(theta)\n",
    "        gradient_mean_sqr = decay * gradient_mean_sqr + (1 - decay) * gradient ** 2  \n",
    "        # **2 means elementwise ^2\n",
    "        theta -= lr * gradient / (np.sqrt(gradient_mean_sqr) + delta)  \n",
    "        # np.sqrt meanselementwise square roots\n",
    "        # / means elementwise division\n",
    "        loss.append(J(theta))\n",
    "    \n",
    "    return theta, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"gradient_mean_sqr\" is always positive, but might have values equal or close to zero. \"delta\" is only there to stabilize the divisions by \"gradient_mean_sqr\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage And Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.25, 0.0625, 0.015625, 0.00390625]\n",
      "[0.0625 0.0625]\n"
     ]
    }
   ],
   "source": [
    "theta_opt1, loss1 = gradient_descent(J, dJ, theta_start, steps = 5, lr = .5)\n",
    "print(loss1)\n",
    "print(theta_opt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17544677397202935, 0.006086804167495761, 0.00012448839791948842, 1.1634232222190242e-06, 2.6593314747211394e-09]\n",
      "[5.15687064e-05 5.15687064e-05]\n"
     ]
    }
   ],
   "source": [
    "theta_opt2, loss2 = rmsprop(J, dJ, theta_start, steps = 5, lr = .5)\n",
    "print(loss2)\n",
    "print(theta_opt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, 5 iterations were done. We can see the loss getting smaller and smaller with each iteration. RMSProp reached a point closer to the saddle point (0,0) compared to Gradient Descent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
